{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment_255_ ANN_Algotithms.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prideven/ANN_Algorithm_255/blob/main/Assignment_255__ANN_Algotithms.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0AdIWWOw9ix_"
      },
      "source": [
        "**Approximate Nearest Neighbour Search**\n",
        "\n",
        "DataSet: MovieLens\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N71_fElG0mCp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "890d6e15-07ad-4ea7-9f28-4fe8c363eac3"
      },
      "source": [
        "#import all the required modules\n",
        "import numpy as np\n",
        "!pip install lightfm\n",
        "from lightfm import LightFM\n",
        "!pip install faiss-gpu\n",
        "import faiss\n",
        "!pip3 install annoy\n",
        "import annoy\n",
        "from annoy import AnnoyIndex\n",
        "!pip install nmslib\n",
        "import nmslib"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting lightfm\n",
            "  Downloading lightfm-1.16.tar.gz (310 kB)\n",
            "\u001b[?25l\r\u001b[K     |█                               | 10 kB 15.6 MB/s eta 0:00:01\r\u001b[K     |██▏                             | 20 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |███▏                            | 30 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |████▎                           | 40 kB 6.5 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 51 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 61 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 71 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 81 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 92 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 102 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 112 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 122 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 133 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 143 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 153 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 163 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 174 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 184 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 194 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 204 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 215 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 225 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 235 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 245 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 256 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 266 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 276 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 286 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 296 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 307 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 310 kB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from lightfm) (1.19.5)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from lightfm) (1.4.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from lightfm) (2.23.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from lightfm) (1.0.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->lightfm) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->lightfm) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->lightfm) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->lightfm) (3.0.4)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->lightfm) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->lightfm) (3.0.0)\n",
            "Building wheels for collected packages: lightfm\n",
            "  Building wheel for lightfm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lightfm: filename=lightfm-1.16-cp37-cp37m-linux_x86_64.whl size=705352 sha256=95b71e2a197e0a505868075fb900f4f0a8ad07ffba921c74c3bb1bfe737ebdfd\n",
            "  Stored in directory: /root/.cache/pip/wheels/f8/56/28/5772a3bd3413d65f03aa452190b00898b680b10028a1021914\n",
            "Successfully built lightfm\n",
            "Installing collected packages: lightfm\n",
            "Successfully installed lightfm-1.16\n",
            "Collecting faiss-gpu\n",
            "  Downloading faiss_gpu-1.7.1.post2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (89.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 89.7 MB 15 kB/s \n",
            "\u001b[?25hInstalling collected packages: faiss-gpu\n",
            "Successfully installed faiss-gpu-1.7.1.post2\n",
            "Collecting annoy\n",
            "  Downloading annoy-1.17.0.tar.gz (646 kB)\n",
            "\u001b[K     |████████████████████████████████| 646 kB 5.3 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: annoy\n",
            "  Building wheel for annoy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for annoy: filename=annoy-1.17.0-cp37-cp37m-linux_x86_64.whl size=391663 sha256=9109c250608c858a15bb4e77dec90842908a9c2fea4bdceca533abe3f866d018\n",
            "  Stored in directory: /root/.cache/pip/wheels/4f/e8/1e/7cc9ebbfa87a3b9f8ba79408d4d31831d67eea918b679a4c07\n",
            "Successfully built annoy\n",
            "Installing collected packages: annoy\n",
            "Successfully installed annoy-1.17.0\n",
            "Collecting nmslib\n",
            "  Downloading nmslib-2.1.1-cp37-cp37m-manylinux2010_x86_64.whl (13.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 13.5 MB 1.5 MB/s \n",
            "\u001b[?25hCollecting pybind11<2.6.2\n",
            "  Downloading pybind11-2.6.1-py2.py3-none-any.whl (188 kB)\n",
            "\u001b[K     |████████████████████████████████| 188 kB 55.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from nmslib) (1.19.5)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from nmslib) (5.4.8)\n",
            "Installing collected packages: pybind11, nmslib\n",
            "Successfully installed nmslib-2.1.1 pybind11-2.6.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDTL5xj79fYH"
      },
      "source": [
        "**Load data**\n",
        "\n",
        "The first step is to get the Movielens data, which is provided by one of the functions provided by LightFM itself."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k8XCR7V64JYb"
      },
      "source": [
        "from lightfm.datasets import fetch_movielens\n",
        "movielens = fetch_movielens()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6PHUsRlHAnaR"
      },
      "source": [
        "**The data consists of dictionary with the following fields:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zu2O3R1p4xDv",
        "outputId": "976079bf-6627-4435-967a-35ad84c57d02"
      },
      "source": [
        "for key, value in movielens.items():\n",
        "    print(key, type(value), value.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train <class 'scipy.sparse.coo.coo_matrix'> (943, 1682)\n",
            "test <class 'scipy.sparse.coo.coo_matrix'> (943, 1682)\n",
            "item_features <class 'scipy.sparse.csr.csr_matrix'> (1682, 1682)\n",
            "item_feature_labels <class 'numpy.ndarray'> (1682,)\n",
            "item_labels <class 'numpy.ndarray'> (1682,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XIVGM-4WA2Wx"
      },
      "source": [
        "**The train and test elements: they contain the raw rating data, split into a train and a test set. Each row represents a user, and each column an item. Entries are ratings from 1 to 5.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UOv4BhYN7mqp",
        "outputId": "b04b1ed5-98e4-43f0-cc70-6c903a90c6a6"
      },
      "source": [
        "model = LightFM(learning_rate=0.05, loss='bpr')\n",
        "model.fit(train, epochs=10)\n",
        "\n",
        "train_precision = precision_at_k(model, train, k=10).mean()\n",
        "test_precision = precision_at_k(model, test, k=10).mean()\n",
        "\n",
        "train_auc = auc_score(model, train).mean()\n",
        "test_auc = auc_score(model, test).mean()\n",
        "\n",
        "print('Precision: train %.2f, test %.2f.' % (train_precision, test_precision))\n",
        "print('AUC: train %.2f, test %.2f.' % (train_auc, test_auc))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: train 0.59, test 0.10.\n",
            "AUC: train 0.89, test 0.86.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UheE62LyBNVP"
      },
      "source": [
        "**The WARP model, on the other hand, optimises for precision@k—we should expect its performance to be better on precision.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dmux0TEb472M",
        "outputId": "53ef8ff0-aae3-45a6-e599-3a54b62c7dc4"
      },
      "source": [
        "model = LightFM(learning_rate=0.05, loss='warp')\n",
        "\n",
        "model.fit_partial(train, epochs=10)\n",
        "\n",
        "train_precision = precision_at_k(model, train, k=10).mean()\n",
        "test_precision = precision_at_k(model, test, k=10).mean()\n",
        "\n",
        "train_auc = auc_score(model, train).mean()\n",
        "test_auc = auc_score(model, test).mean()\n",
        "\n",
        "print('Precision: train %.2f, test %.2f.' % (train_precision, test_precision))\n",
        "print('AUC: train %.2f, test %.2f.' % (train_auc, test_auc))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: train 0.60, test 0.11.\n",
            "AUC: train 0.93, test 0.90.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVZlpDkR5BgS"
      },
      "source": [
        "with open('movielens.pickle', 'wb') as f:\n",
        "    pickle.dump({\"name\": movielens['item_feature_labels'], \"vector\": item_vectors}, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-RgZKT6k6rQe",
        "outputId": "d3ab3e8f-3f6b-434d-a1b9-a78506f1e769"
      },
      "source": [
        "def load_data():\n",
        "    with open('movielens.pickle', 'rb') as f:\n",
        "        data = pickle.load(f)\n",
        "    return data\n",
        "data = load_data()\n",
        "data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'name': array(['Toy Story (1995)', 'GoldenEye (1995)', 'Four Rooms (1995)', ...,\n",
              "        'Sliding Doors (1998)', 'You So Crazy (1994)',\n",
              "        'Scream of Stone (Schrei aus Stein) (1991)'], dtype=object),\n",
              " 'vector': array([[-0.5100154 ,  0.12129407, -0.15239356, ...,  0.40692097,\n",
              "          0.42848933, -0.13139375],\n",
              "        [ 0.32232195, -0.5089782 ,  0.51367533, ..., -0.02939505,\n",
              "          0.10195416,  0.34751478],\n",
              "        [ 0.19888787,  0.00631484, -0.3133142 , ...,  0.5857541 ,\n",
              "          0.4684008 , -0.16707247],\n",
              "        ...,\n",
              "        [ 0.10898031,  0.13037936, -0.00164933, ...,  0.04329937,\n",
              "         -0.02095714, -0.14370337],\n",
              "        [-0.00648901, -0.04058164,  0.01574951, ...,  0.04156749,\n",
              "          0.04157975, -0.03762632],\n",
              "        [ 0.08128444, -0.00384731, -0.07413117, ...,  0.04755254,\n",
              "          0.11984958,  0.04443109]], dtype=float32)}"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUtPaERJCFXA"
      },
      "source": [
        "**Exhaustive Search**\n",
        "\n",
        "The exhaustive search also know as  brute-force search, is a very general problem-solving technique and algorithmic paradigm that consists of systematically enumerating all possible iterations for the solution and checking whether each iteration satisfies the problem's statement.\n",
        "even though simple in it has high time complexity.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LNst9az_CNp6"
      },
      "source": [
        "class ExhaustiveIndex():\n",
        "    def __init__(self, vectors, labels):\n",
        "        self.dimension = vectors.shape[1]\n",
        "        self.vectors = vectors.astype('float32')\n",
        "        self.labels = labels    \n",
        "   \n",
        "    def build(self):\n",
        "        self.index = faiss.IndexFlatL2(self.dimension,)\n",
        "        self.index.add(self.vectors)\n",
        "        \n",
        "    def query(self, vectors, k=10):\n",
        "        distances, indices = self.index.search(vectors, k) \n",
        "        return [self.labels[i] for i in indices[0]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5dDij_RQCREl"
      },
      "source": [
        "index = ExhaustiveIndex(data[\"vector\"], data[\"name\"])\n",
        "index.build()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lsU704AXCWyz",
        "outputId": "87c5df5b-903b-473c-d4e8-cfd99d8c1048"
      },
      "source": [
        "movie_vector, movie_name = data['vector'][50:51], data['name'][50]\n",
        "simlar_movies = '\\n--> '.join(index.query(movie_vector))\n",
        "print(f\"The most similar movie to {movie_name} are:\\n--> {simlar_movies}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The most similar movie to Legends of the Fall (1994) are:\n",
            "--> Legends of the Fall (1994)\n",
            "--> Boys on the Side (1995)\n",
            "--> Powder (1995)\n",
            "--> Rudy (1993)\n",
            "--> When a Man Loves a Woman (1994)\n",
            "--> Nell (1994)\n",
            "--> Poetic Justice (1993)\n",
            "--> I'll Do Anything (1994)\n",
            "--> Corrina, Corrina (1994)\n",
            "--> Client, The (1994)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkA-zeil7IxM"
      },
      "source": [
        "**Locality_Sensitive_Hashing (LSH)**\n",
        "\n",
        "Locality Sensitive Hashing (LSH) refers to a set of algorithmic techniques used to speed up the search for neighbours or duplicate data in the samples. LSH can be used to filter out duplicates in a database, scrape web pages or any websites at an impressive speed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aCHeQZuX7TOA"
      },
      "source": [
        "class LSHIndex():\n",
        "    def __init__(self, vectors, labels):\n",
        "        self.dimension = vectors.shape[1]\n",
        "        self.vectors = vectors.astype('float32')\n",
        "        self.labels = labels    \n",
        "   \n",
        "    def build(self, num_bits=8):\n",
        "        self.index = faiss.IndexLSH(self.dimension, num_bits)\n",
        "        self.index.add(self.vectors)\n",
        "        \n",
        "    def query(self, vectors, k=10):\n",
        "        distances, indices = self.index.search(vectors, k) \n",
        "        return [self.labels[i] for i in indices[0]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uk_Ct5p-7V7n"
      },
      "source": [
        "index = LSHIndex(data[\"vector\"], data[\"name\"])\n",
        "index.build()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H0tvukVr7YJK",
        "outputId": "9d18266b-f493-42fb-fd98-02d47343e92f"
      },
      "source": [
        "movie_vector, movie_name = data['vector'][50:51], data['name'][50]\n",
        "simlar_movies = '\\n--> '.join(index.query(movie_vector))\n",
        "print(f\"The most similar movies to {movie_name} are:\\n--> {simlar_movies}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The most similar movies to Legends of the Fall (1994) are:\n",
            "--> Legends of the Fall (1994)\n",
            "--> Boys on the Side (1995)\n",
            "--> Powder (1995)\n",
            "--> Rudy (1993)\n",
            "--> When a Man Loves a Woman (1994)\n",
            "--> Nell (1994)\n",
            "--> Poetic Justice (1993)\n",
            "--> I'll Do Anything (1994)\n",
            "--> Corrina, Corrina (1994)\n",
            "--> Client, The (1994)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxvhabc27w8I"
      },
      "source": [
        "**Trees and Graphs (Annoy)**\n",
        "\n",
        "An Annoy index consists of N binary trees, where each tree partitions the vector space using random hyperplanes at each node in the tree."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygo6ZGTc7kyf"
      },
      "source": [
        "class TreesIndex():\n",
        "    def __init__(self, vectors, labels):\n",
        "        self.dimention = vectors.shape[1]\n",
        "        self.vectors = vectors.astype('float32')\n",
        "        self.labels = labels\n",
        "\n",
        "\n",
        "    def build(self, number_of_trees=5):\n",
        "        self.index = annoy.AnnoyIndex(self.dimention)\n",
        "        for i, vec in enumerate(self.vectors):\n",
        "            self.index.add_item(i, vec.tolist())\n",
        "        self.index.build(number_of_trees)\n",
        "        \n",
        "    def query(self, vector, k=10):\n",
        "        indices = self.index.get_nns_by_vector(vector.tolist(), k)\n",
        "        return [self.labels[i] for i in indices]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Vv298o7721A",
        "outputId": "8e13d043-5805-4fb9-8247-fd4bfd404f51"
      },
      "source": [
        "\n",
        "index = TreesIndex(data[\"vector\"], data[\"name\"])\n",
        "index.build()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: FutureWarning: The default argument for metric will be removed in future version of Annoy. Please pass metric='angular' explicitly.\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SqFZRDhu7-41",
        "outputId": "2431d0b9-6368-4d18-f057-edc06708b83f"
      },
      "source": [
        "movie_vector, movie_name = data['vector'][50], data['name'][50]\n",
        "similar_movies = '\\n--> '.join(index.query(movie_vector))\n",
        "print(f\"The most similar movie to {movie_name} are:\\n--> {simlar_movies}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The most similar movie to Legends of the Fall (1994) are:\n",
            "--> Legends of the Fall (1994)\n",
            "--> Boys on the Side (1995)\n",
            "--> Powder (1995)\n",
            "--> Rudy (1993)\n",
            "--> When a Man Loves a Woman (1994)\n",
            "--> Nell (1994)\n",
            "--> Poetic Justice (1993)\n",
            "--> I'll Do Anything (1994)\n",
            "--> Corrina, Corrina (1994)\n",
            "--> Client, The (1994)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jzve5t-D8lTz"
      },
      "source": [
        "**HNSW**\n",
        "\n",
        "HNSW (Hierarchical Navigable Small World Algorithm) builds a hierarchical graph incrementally, and has great search performance with high recall, motivating us to prototype it for comparison. Each node in the graph represents a point in the vector space, and nodes are linked to other nodes that are close in space."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1mw19eD28jdy"
      },
      "source": [
        "class HNSWIndex():\n",
        "    def __init__(self, vectors, labels):\n",
        "        self.dimention = vectors.shape[1]\n",
        "        self.vectors = vectors.astype('float32')\n",
        "        self.labels = labels\n",
        "    def build(self):\n",
        "        self.index = nmslib.init(method='hnsw', space='cosinesimil')\n",
        "        self.index.addDataPointBatch(self.vectors)\n",
        "        self.index.createIndex({'post': 2})\n",
        "        \n",
        "    def query(self, vector, k=10):\n",
        "        indices = self.index.knnQuery(vector, k=k)\n",
        "        return [self.labels[i] for i in indices[0]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ONG-kIN8tWg"
      },
      "source": [
        "index = HNSWIndex(data[\"vector\"], data[\"name\"])\n",
        "index.build()"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pd8dnCjt8ved",
        "outputId": "21502875-1da1-4c71-cb41-6b63d44b5eef"
      },
      "source": [
        "movie_vector, movie_name = data['vector'][50], data['name'][50]\n",
        "simlar_movies = '\\n--> '.join(index.query(movie_vector))\n",
        "print(f\"The most similar stack to {movie_name} are:\\n--> {simlar_movies}\")"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The most similar stack to Legends of the Fall (1994) are:\n",
            "--> Legends of the Fall (1994)\n",
            "--> Jason's Lyric (1994)\n",
            "--> Boys on the Side (1995)\n",
            "--> When a Man Loves a Woman (1994)\n",
            "--> Ghost (1990)\n",
            "--> I'll Do Anything (1994)\n",
            "--> Powder (1995)\n",
            "--> American President, The (1995)\n",
            "--> Poetic Justice (1993)\n",
            "--> Rudy (1993)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3DEaazv9f_B"
      },
      "source": [
        "**Product** **Quantization**\n",
        "\n",
        "Works on the idea to decomposes the space into a Cartesian\n",
        "product of low dimensional subspaces and to quantize each\n",
        "subspace separately. A vector is represented by a short\n",
        "code composed of its subspace quantization indices. The\n",
        "Euclidean distance between two vectors can be efficiently\n",
        "estimated from their code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSVOFG9H866l"
      },
      "source": [
        "class ProductIndex():\n",
        "    def __init__(self, vectors, labels):\n",
        "        self.dimension = vectors.shape[1]\n",
        "        self.vectors = vectors.astype('float32')\n",
        "        self.labels = labels    \n",
        "    def build(self, number_of_partition=8, search_in_x_partitions=2, subvector_size=8):\n",
        "        quantizer = faiss.IndexFlatL2(self.dimension)\n",
        "        self.index = faiss.IndexIVFPQ(quantizer, \n",
        "                                      self.dimension, \n",
        "                                      number_of_partition, \n",
        "                                      search_in_x_partitions, \n",
        "                                      subvector_size)\n",
        "        self.index.train(self.vectors)\n",
        "        self.index.add(self.vectors)\n",
        "        \n",
        "    def query(self, vectors, k=10):\n",
        "        distances, indices = self.index.search(vectors, k) \n",
        "        # I expect only query on one vector thus the slice\n",
        "        return [self.labels[i] for i in indices[0]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UlNXI1kM90yO"
      },
      "source": [
        "index = ProductIndex(data[\"vector\"], data[\"name\"])\n",
        "index.build()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I6YbBdTm94DO",
        "outputId": "a1a66444-4af2-4e92-de9c-fd40436da923"
      },
      "source": [
        "movie_index = 50\n",
        "movie_vector = data['vector'][movie_index:movie_index+1]\n",
        "print(f\"The most simillar movie to {data['name'][movie_index]} are:\")\n",
        "index.query(movie_vector)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The most simillar movie to Legends of the Fall (1994) are:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Legends of the Fall (1994)',\n",
              " \"Jason's Lyric (1994)\",\n",
              " 'Boys on the Side (1995)',\n",
              " 'When a Man Loves a Woman (1994)',\n",
              " 'Ghost (1990)',\n",
              " \"I'll Do Anything (1994)\",\n",
              " 'Powder (1995)',\n",
              " 'American President, The (1995)',\n",
              " 'Poetic Justice (1993)',\n",
              " 'Rudy (1993)']"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ahTZYmAbPuAG"
      },
      "source": [
        "\n",
        "References:\n",
        "1. https://making.lyst.com/lightfm/docs/examples/movielens_implicit.html\n",
        "2. https://blog.vespa.ai/approximate-nearest-neighbor-search-in-vespa-part-1/"
      ]
    }
  ]
}